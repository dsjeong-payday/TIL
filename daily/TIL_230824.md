# AI/DL
## NLP Course - Hugging Face
### 3. FINE-TUNING A PRETRAINED MODEL
#### Processing the data
아래는 checkpoint 모델에 추가적으로 문장을 훈련시킨 코드.
```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# This is new
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```
##### Loading a dataset from the Hub
- [HF 데이터셋 허브](https://huggingface.co/datasets)
- HF 데이터셋 라이브러리 `datasets`
```python
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets

# DatasetDict({
#     train: Dataset({
#         features: ['sentence1', 'sentence2', 'label', 'idx'],
#         num_rows: 3668
#     })
#     validation: Dataset({
#         features: ['sentence1', 'sentence2', 'label', 'idx'],
#         num_rows: 408
#     })
#     test: Dataset({
#         features: ['sentence1', 'sentence2', 'label', 'idx'],
#         num_rows: 1725
#     })
# })
```
`~/.cache/huggingface/datasets` 경로에 데이터셋 캐싱함
```python
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]

# {'idx': 0,
#  'label': 1,
#  'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
#  'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```