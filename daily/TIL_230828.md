# AI/DL
## NLP Course - Hugging Face
### 3. FINE-TUNING A PRETRAINED MODEL
#### Processing the data
##### Preprocessing a dataset
토크나이저: 문자열을 여러 토큰으로 분해, 각 토큰은 정수에 할당
```python
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```
두 문장을 쌍으로 처리
```python
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs

# { 
#   'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
#   'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
#   'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
# }
```
`token_type_ids`: 각각의 토큰이 몇 번째 문장에 속한 것인지 구분
```python
tokenizer.convert_ids_to_tokens(inputs["input_ids"])

# ['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
# [      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```
`Dataset.map()`: 특정 함수를 데이터셋의 각 요소에 적용. 예시)
```python
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```
이때 `padding` 인자를 생략했는데, 모든 문장에 `padding` 최대 길이를 적용하는 것은 비효율적이기 때문. 대신에 배치를 만들 때 해당 배치의 길이에 맞추어 패딩하는 것이 낫다.

```python
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets

# DatasetDict({
#     train: Dataset({
#         features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
#         num_rows: 3668
#     })
#     validation: Dataset({
#         features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
#         num_rows: 408
#     })
#     test: Dataset({
#         features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
#         num_rows: 1725
#     })
# })
```
`map()` 함수를 호출할 때, `num_proc` 인자를 넣으면 멀티 프로세싱 가능. 다만 위의 코드에 쓰인 `HuggingFace tokenizer` 라이브러리는 백엔드에 Rust를 사용하며 기본적으로 멀티 프로세싱을 지원함.