# AI/DL
## NLP Course - Hugging Face
### 1. Transformer Models
#### How do Transformers work?
트랜스포머 작동 원리.
- 트랜스포머 등장: Vaswani et al. (2017) [Attention is All You Need](https://arxiv.org/abs/1706.03762)
- 주류 패밀리 3종: GPT(*auto-regressive*), BERT(*auto-encoding*), BART/T5(*sequence-to-sequence*)
- self-supervised learning with large amount of raw texts
- 언어 모델: 언어에 대한 통계학적인 이해도 높지만, 특정한 과업에는 취약할 수 있음.
- *transfer learning*: 사전 학습된 언어 모델을 지도학습 방법으로 파인 튜닝함으로써 특정한 과업을 해결하는 모델로 조정하는 기법
- 언어 모델 학습비용은 천문학적. 환경 부담...사전학습된 모델을 돌려쓰는 것이 효율적 (Hugging Face Model Hub mission)

##### Encoder-Decoder
Encoder: 입력값을 받아서 feature representation 만듦. understanding에 최적화
Decoder: Encoder가 전달한 feature로부터 출력값 생성. generating에 최적화
해결하고자 하는 문제에 따라서 독립적으로 활용할 수 있음 
- **Encoder-only models**: 문장 분류, NER 등
- **Decoder-only models**: 텍스트 생성 등
- **Encoder-decoder models (sequence-to-sequence models)**: 번역, 요약 등

##### Attention layers
입력 텍스트 중에서 특정한 단어에 주목하도록 하는 레이어
아래는 불어-영어 번역 문제에서 어텐션 레이어의 대략적인 역할
> To put this into context, consider the task of translating text from English to French. Given the input “You like this course”, a translation model will need to also attend to the adjacent word “You” to get the proper translation for the word “like”, because in French the verb “like” is conjugated differently depending on the subject. The rest of the sentence, however, is not useful for the translation of that word. In the same vein, when translating “this” the model will also need to pay attention to the word “course”, because “this” translates differently depending on whether the associated noun is masculine or feminine. Again, the other words in the sentence will not matter for the translation of “this”. With more complex sentences (and more complex grammar rules), the model would need to pay special attention to words that might appear farther away in the sentence to properly translate each word.

단어 자체도 의미를 갖지만, 그 의미는 문맥에 좌우
자연어처리에서의 문맥이란 해당 단어의 앞뒤 단어로부터 추론

##### Transformer's original architecture
Transformer 모델의 당초 해결 목표는 번역이었음
훈련 - encoder가 특정 언어의 입력값을 받고, decoder가 동일한 의미의 문장을 목표 언어로 받음.
Encoder의 어텐션 레이어는 문장의 모든 단어를 사용할 수 있는 반면, decoder는 순차적으로 작동하며 이미 번역된 앞부분의 단어만을 주목할 수 있음.
예) 세 번째 단어까지 생성한 상황에서 네 번째 단어의 예측은 encoder가 받은 전체 문장과 decoder가 생성한 세 단어를 활용함
훈련 과정에서 decoder는 목표 언어의 문장을 통째로 투입받긴 하지만, 아직 도달하지 않은 부분까지는 추론에 활용할 수 없게 함.
![Alt text](../imgs/transformers.svg "Transformer's original architecture")
decoder의 첫 번째 어텐션 레이어는 decoder에 대한 입력값에 주목하지만, 두 번째 레이어는 encoder의 산출값을 활용함. 다음 번 단어를 예측할 떄마다 입력 문장의 전체 구조에 접근한다는 의미. 서로 다른 언어 사이의 문법적인 규칙을 반영할 수 있음
그밖에 *attention mask*: padding과 같은 요소에 주목하지 않도록 할 수 있음.

Model 용어
- **architecture**: 모델의 뼈대. 신경망 내부의 레이어 구조와 연산 (예: BERT)
- **checkpoints**: 가중치. 모델의 architecture에 탑재될 값들 (예: `bert-base-cased`)
- **model**: 맥락에 따라 architecture를 가리킬 수도, checkpoints를 가리킬 수도 있음

#### 요약
| Model       | Examples    | Tasks       |
| ----------- | ----------- | ----------- |
| Encoder      | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa	| 문장분류, NER, Extractive 문답 |
| Decoder   | CTRL, GPT, GPT-2, Transformer XL	        | 문장 생성 |
| Encoder-Decoder   | BART, T5, Marian, mBART	        | 요약, 번역, Generative 문답 |