# AI/DL
## NLP Course - Hugging Face
### 3. FINE-TUNING A PRETRAINED MODEL
#### Processing the data
##### Dynamic padding
*collate function*: 여러 샘플을 하나의 배치에 넣는 함수. (collate: collect and combine in the correct order.) `DataLoader`의 인자로 전달 가능. 
`DataCollatorWithPadding`: 배치에 투입할 샘플을 동일한 크기로 제공해야 할 때, 각각의 항목에 맞는 패딩을 입히는 함수.
```python
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
동적 패딩: 아래와 같이 8개 문장을 하나의 배치로 넣을 때 모든 샘플의 길이를 배치 내 최대값(=67)과 동일하게끔 패딩하기. 이와 달리 동적 패딩이 없으면 전체 샘플의 최대 길이나 모델이 받아들일 수 있는 최대값에 맞추어 패딩하게 됨.

```python
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
# [50, 59, 47, 67, 59, 50, 62, 32]
```
위에서 정의한 `data_collator`도 마찬가지로 동적 패딩을 적용하고 있음.
```python
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}

# {'attention_mask': torch.Size([8, 67]),
#  'input_ids': torch.Size([8, 67]),
#  'token_type_ids': torch.Size([8, 67]),
#  'labels': torch.Size([8])}
```
#### Fine-tuning a model with the Trainer API
`Trainer` 클래스: 사전 학습된 모델을 추가 학습.
```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
##### Training
##### Evaluation