# AI/DL
## NLP Course - Hugging Face
### 2. Using Transformers
#### Introduction
Transformer ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°œë°œ ì·¨ì§€: ì—¬ëŸ¬ íŠ¸ëžœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ê°„íŽ¸í•˜ê²Œ ë¶ˆëŸ¬ì˜¤ê³ , í›ˆë ¨í•˜ê³ , ì €ìž¥í•  ìˆ˜ ìžˆëŠ” API ì œê³µ
- **Ease of use**: Hubì— ê³µê°œëœ NLP ëª¨ë¸ì„ ë‹¨ ë‘ ì¤„ì˜ ì½”ë“œë¡œ ì‚¬ìš© ê°€ëŠ¥
- **Flexibility**: PyTorch `nn.Module` í˜¹ì€ TensorFlow `tf.keras.Model` í´ëž˜ìŠ¤ë¡œ ì œê³µ
- **Simplicity**: "All in one file". ëª¨ë¸ì„ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ í™œìš©í•˜ê¸°. ì½”ë“œê°€ ë‹¨ìˆœí•´ì§€ê¸°ë„ í•˜ê³  ëª¨ë¸ë¼ë¦¬ ë¶„ë¦¬ë˜ê¸°ë„ í•¨
#### Behind the pipeline
![Alt text](../imgs/full_nlp_pipeline.svg "Transformer's original architecture")
##### Tokenizer: raw textë¥¼ ìžì—°ì–´ì²˜ë¦¬ ëª¨ë¸ì´ ë‹¤ë£° ìˆ˜ ìžˆëŠ” ìˆ«ìžë¡œ ë°”ê¾¸ëŠ” ì—­í•  (í† í°í™”)
- í…ìŠ¤íŠ¸ ìž…ë ¥ê°’ì„ ë‹¨ì–´, subwords, ê¸°í˜¸ ë“± *í† í°*ìœ¼ë¡œ ìª¼ê°¬
- ê°ê°ì˜ *í† í°*ì„ ì–´ëŠ ì •ìˆ˜ì— ë§¤í•‘
- ëª¨ë¸ì— ìœ ìš©í•œ ìž…ë ¥ê°’ ì¶”ê°€

`AutoTokenizer.from_pretrained('ì²´í¬í¬ì¸íŠ¸ëª…')`: ì²´í¬í¬ì¸íŠ¸ë¡œë¶€í„° tokenizer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
```python
from transformers import AutoTokenizer

# ì²´í¬í¬ì¸íŠ¸ ëª…ì‹œ
checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"

# ìœ„ì˜ ì²´í¬í¬ì¸íŠ¸ë¡œë¶€í„° tokenizer í˜¸ì¶œ
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# tokenizerì— íˆ¬ìž…í•  ìž…ë ¥ê°’
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]

# inputs: í…ìŠ¤íŠ¸ ìž…ë ¥ê°’ì„ í† í°í™”í•œ ê²°ê³¼ -> ì¶”í›„ ëª¨ë¸ì˜ ìž…ë ¥ê°’ìœ¼ë¡œ íˆ¬ìž…ë  ì˜ˆì •
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```
ì•„ëž˜ëŠ” ìœ„ì˜ `raw_inputs`ì˜ í† í°í™”ëœ ë²„ì „
```
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```
`input_ids`: ë‘ ê°œì˜ ì •ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹´ì€ `tensor`. ê°ê° ë¦¬ìŠ¤íŠ¸ëŠ” `raw_inputs`ì˜ ë¬¸ìž¥ê³¼ ëŒ€ì‘í•˜ë©°, ê°ê°ì˜ ì •ìˆ˜ëŠ” `token`ì˜ ì‹ë³„ìžìž„.
##### Model - `token` -> ê³ ì°¨ì› ë²¡í„° -> head -> outputs
`AutoModel.from_pretrained('ì²´í¬í¬ì¸íŠ¸ëª…')`: `token` ìž…ë ¥ì„ ë°›ì•„ì„œ íŠ¹ì •í•œ ì˜ˆì¸¡ ê³¼ì • ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸.
```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```

ì˜ˆì œì˜ ëª¨ë¸ì€ ì£¼ì–´ì§„ `token`ì— ëŒ€í•´ *hidden states* (í˜¹ì€ *features*) ë°˜í™˜í•˜ëŠ” êµ¬ì¡°. ì—¬ê¸°ì„œ *hidden states*ëŠ” **ê³ ì°¨ì› ë²¡í„°**ì˜ í˜•íƒœë¡œ í‘œí˜„í•¨. input -> contextual understanding by the Transformer model

ê³ ì°¨ì› ë²¡í„°ì˜ í˜•íƒœë¥¼ êµ¬ë¶„ì§“ëŠ” 3ëŒ€ ë³€ìˆ˜
- **Batch size**: í•œ ë²ˆì— ì²˜ë¦¬ë˜ëŠ” ì‹œí€€ìŠ¤ì˜ ìˆ˜
- **Sequence length**: ì‹œí€€ìŠ¤ì˜ ìˆ«ìž í‘œí˜„ì˜ ê¸¸ì´
- **Hidden size**: ëª¨ë¸ ìž…ë ¥ê°’ì˜ ë²¡í„° ì°¨ì› í¬ê¸°(768ì—ì„œ í° ëª¨ë¸ì€ 3,072 ì´ìƒ)
```python
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)
# => torch.Size([2, 16, 768])
```

Model Heads
![Alt text](../imgs/transformer_and_head.svg "Transformer's original architecture")
*Hidden states*ë¥¼ ìž…ë ¥ê°’ìœ¼ë¡œ ë°›ì•„ì„œ ë‹¤ë¥¸ ì°¨ì›ì˜ ë²¡í„°ë¡œ ë°˜í™˜í•˜ëŠ” ì—­í• . ì£¼ë¡œ í•œ ê°œ, í˜¹ì€ ëª‡ ê°œì˜ ì„ í˜• ë ˆì´ì–´ë¡œ ì´ë£¨ì–´ì§. ì–¸ì–´ì™€ ë§¥ë½ì— ëŒ€í•œ ì¶”ìƒì ì¸ ë²¡í„°ë¡œë¶€í„° ëª¨ë¸ì„ í†µí•´ í•´ê²°í•˜ê³ ìž í•˜ëŠ” ë¬¸ì œì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •. 

ì•„ëž˜ëŠ” Transformer ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì œê³µí•˜ëŠ” ì•„í‚¤í…ì²˜ ì˜ˆì‹œ:
- *Model (retrieve the hidden states)
- *ForCausalLM
- *ForMaskedLM
- *ForMultipleChoice
- *ForQuestionAnswering
- *ForSequenceClassification
- *ForTokenClassification
- and others ðŸ¤—

ë¬¸ìž¥ë¶„ë¥˜ ë¬¸ì œ ì˜ˆì œ
```python
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)

print(outputs.logits.shape)
# => torch.Size([2, 2]) 2ê°œì˜ ë¬¸ìž¥ê³¼ 2ê°œì˜ label
```
##### Postprocessing the output
2x2 ì¶œë ¥ê°’ì˜ ì •ì²´ëŠ” *logits* ê°’
```python
print(outputs.logits)
# => tensor([[-1.5607,  1.6123],
#            [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)
```
*logits* í•¨ìˆ˜ ê°’ì„ 0ì—ì„œ 1ì‚¬ì´ì˜ í™•ë¥  ê°’ìœ¼ë¡œ ë³€í™˜í•˜ë ¤ë©´ SoftMax ë ˆì´ì–´ì— í†µê³¼ì‹œì¼œì•¼ í•¨
```python
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
# => tensor([[4.0195e-02, 9.5980e-01],
#            [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)
```
ì²« ë²ˆì§¸ ë¬¸ìž¥ì— ëŒ€í•œ ê²°ê³¼ê°’ì€ [.0402, .9598]ë¡œì„œ ë‘ ë²ˆì§¸ labelì— ëŒ€í•´ 95.98%ì˜ í™•ë¥  ë°˜í™˜. ë‘ ë²ˆì§¸ ë¬¸ìž¥ì— ëŒ€í•œ ê²°ê³¼ê°’ì€ [.9995, .0005]. ê° label ëª…ì¹­ì„ í™•ì¸í•˜ê³ ìž í•œë‹¤ë©´, `model.config.id2label` ì¡°íšŒí•´ë³¼ ê²ƒ
```python
print(model.config.id2label)
# => {0: 'NEGATIVE', 1: 'POSITIVE'}
```

| ë¬¸ìž¥ | NEGATIVE(=0) | POSITIVE(=1) |
| ------ | ------ | ------ |
| (1) | .0402 | .9598 |
| (2) | .9995 | .0005 |
